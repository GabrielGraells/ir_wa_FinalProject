{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF-IDF_SearchEngine.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP52tHrKRQGClx3JQnxwWbN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BTblnWXg8sKl"},"source":["\n","# TF-IDF + Cosine Similarity"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iiIaSpOX8uzv","executionInfo":{"status":"ok","timestamp":1607549074404,"user_tz":-60,"elapsed":653,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}},"outputId":"4b7c3a87-3756-475e-8983-ebd3ebaced81"},"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from array import array\n","import collections\n","from collections import defaultdict\n","import string\n","import pickle\n","\n","nltk.download('stopwords')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tKGJBhwsJP__"},"source":["## Import Files"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":197},"id":"nc5MPWpEJTt5","executionInfo":{"status":"ok","timestamp":1607549077152,"user_tz":-60,"elapsed":1999,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}},"outputId":"9ef81423-07da-4619-b61c-36cc34a37408"},"source":["data = pd.read_csv(f\"{PATH}final_Tweets.csv\")\n","data.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>created_at</th>\n","      <th>entities.hashtags</th>\n","      <th>favorite_count</th>\n","      <th>full_text</th>\n","      <th>id</th>\n","      <th>retweet_count</th>\n","      <th>user.id</th>\n","      <th>user.name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020-11-11</td>\n","      <td>[]</td>\n","      <td>4</td>\n","      <td>International friendly roundup: Finland stun F...</td>\n","      <td>1326667371730378753</td>\n","      <td>1</td>\n","      <td>16042794</td>\n","      <td>Guardian US</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2020-11-11</td>\n","      <td>[]</td>\n","      <td>11</td>\n","      <td>When Joe Biden formally takes over the preside...</td>\n","      <td>1326666012142526466</td>\n","      <td>5</td>\n","      <td>16042794</td>\n","      <td>Guardian US</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2020-11-11</td>\n","      <td>[]</td>\n","      <td>4</td>\n","      <td>New Yorker fires Jeffrey Toobin after he repor...</td>\n","      <td>1326663505454510081</td>\n","      <td>1</td>\n","      <td>16042794</td>\n","      <td>Guardian US</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2020-11-11</td>\n","      <td>[]</td>\n","      <td>8</td>\n","      <td>One week on: how has Donald Trump handled losi...</td>\n","      <td>1326661105498796032</td>\n","      <td>1</td>\n","      <td>16042794</td>\n","      <td>Guardian US</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2020-11-11</td>\n","      <td>[]</td>\n","      <td>13</td>\n","      <td>France pays tribute to six-year-old resistance...</td>\n","      <td>1326659924278046728</td>\n","      <td>6</td>\n","      <td>16042794</td>\n","      <td>Guardian US</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   created_at entities.hashtags  ...   user.id    user.name\n","0  2020-11-11                []  ...  16042794  Guardian US\n","1  2020-11-11                []  ...  16042794  Guardian US\n","2  2020-11-11                []  ...  16042794  Guardian US\n","3  2020-11-11                []  ...  16042794  Guardian US\n","4  2020-11-11                []  ...  16042794  Guardian US\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"fk241Aur891q"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"id":"1gjVeUSL82yc","executionInfo":{"status":"ok","timestamp":1607549077153,"user_tz":-60,"elapsed":1645,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["\n","def getTerms(terms):\n","\n","  \"\"\"\n","  Preprocess step for tweet.\n","    1. transform all text in lowercase\n","    2. Tokenize by transforming string to list\n","    3. Remove stopwords\n","    4. Apply stemming, simplify word to root word.\n","  \n","  Argument:\n","    terms -- string (text)\n","  \n","  Returns:\n","    terms -- a list of preprocessed tokens corresponding to the input tweet \n","  \"\"\"\n","  # Init stemming and stopwords\n","  stemming = PorterStemmer()\n","  STOPWORDS = set(stopwords.words(\"english\"))\n","\n","  # Transform into lower case\n","  terms = terms.lower() \n","\n","  # Remove punctuation\n","  terms = terms.translate(str.maketrans(\"\",\"\", string.punctuation))\n","\n","  # Tokenize\n","  terms = terms.split()\n","\n","  # Remove stop words\n","  terms = [t for t in terms if t not in STOPWORDS]\n","\n","  #Stemming\n","  terms = [stemming.stem(t) for t in terms]\n","\n","  # Remove http links found in tweets last term\n","  try:\n","    if terms[-1][:4] == \"http\":\n","     terms = terms[:-1]\n","  except:\n","    pass\n","\n","  return terms"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rMzp3KL4b8If"},"source":["# TF-IDF Inverted Index"]},{"cell_type":"code","metadata":{"id":"AVPa69_dT8Xz","executionInfo":{"status":"ok","timestamp":1607549077154,"user_tz":-60,"elapsed":1272,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["def create_index_tfidf(data, numDocs):\n","  \"\"\"\n","    Creates index and computes tf, idf and df for collection\n","\n","    Arguments:\n","      data -- Pandas dataframe with cols \"full_index\" per text tweet and \"id\" per \n","              tweet-\n","    Returns:\n","      index --  inverted list \"term\": [[\"id\",[pos1,pos1,..]]..]\n","      tf -- normalized term frequency per doc \n","      df -- document frequency per term\n","      idf -- inversed docuemnt frequency\n","  \"\"\"\n","\n","  index = defaultdict(list)\n","\n","  # Term freq of terms in tweets\n","  tf = defaultdict(list)\n","\n","  # Tweet freq of term in corpus\n","  df = defaultdict(int)\n","\n","  # Inverse df\n","  idf = defaultdict(float)\n","\n","  for i, row in data.iterrows():\n","    tweet = row[\"full_text\"]\n","    tweet = getTerms(tweet)\n","    tweet_id = row[\"id\"]\n","\n","    # Page Index per doc\n","    termdictPage = {}\n","    for position, term in enumerate(tweet):\n","      try:\n","        termdictPage[term][1].append(position)\n","      except:\n","        termdictPage[term] = [tweet_id, array(\"I\",[position])]\n","    \n","    # NORM frequencies per doc \n","    norm = 0\n","    for term, posting in termdictPage.items():\n","      norm += len(posting[1])**2\n","    norm = np.sqrt(norm)\n","\n","    # TF normalized per term and DF\n","    for term, posting in termdictPage.items():\n","      tf[term].append(np.round(len(posting[1])/norm,4))\n","      df[term] += 1\n","    \n","    # Merge doc page index with main index\n","    for termpage, postingpage in termdictPage.items():\n","      index[termpage].append(postingpage)\n","\n","    # IDF per term\n","    for term in df:\n","      idf[term] = np.round(np.log(float(numDocs/df[term])),4)\n","  \n","  return index, tf, df, idf"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8xFn_5BTRb4l","executionInfo":{"status":"ok","timestamp":1607550103550,"user_tz":-60,"elapsed":1027475,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}},"outputId":"dcb8c30a-d5af-4db4-ce1e-f70935e2e3ec"},"source":["%%time\n","\n","numDocs = len(data)\n","index, tf, df, idf = create_index_tfidf(data, numDocs)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["CPU times: user 17min 2s, sys: 2.07 s, total: 17min 4s\n","Wall time: 17min 6s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VyPQjvtBCO4e","executionInfo":{"status":"ok","timestamp":1607550106362,"user_tz":-60,"elapsed":1030091,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["# Save results\n","pickle.dump(index,open(f\"{PATH}utils/index.p\", \"wb\"))\n","pickle.dump(tf, open(f\"{PATH}utils/tf.p\", \"wb\"))\n","pickle.dump(df,open(f\"{PATH}utils/df.p\", \"wb\"))\n","pickle.dump(idf,open(f\"{PATH}utils/idf.p\", \"wb\"))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FmYO30s5bhGA"},"source":["# Ranking"]},{"cell_type":"code","metadata":{"id":"d3DQLK0xUQsY","executionInfo":{"status":"ok","timestamp":1607550106365,"user_tz":-60,"elapsed":1029694,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["def rankDocuments(terms, docs, index, idf, tf):\n","  \"\"\"\n","  Computes ranking given query and collection of tweets.\n","\n","  Arguments:\n","    terms -- query - str.\n","    docs -- ID list of docs - list.\n","    index -- invertex index. - dict\n","    idf -- inverse document frequency - dict\n","    tf -- term frequency - dict\n","  Returns:\n","    resultDocs -- Ordered list of matching docs based on cosine-sim - list\n","  \"\"\"\n","\n","  # Dict with vector per docID\n","  docVectors = defaultdict(lambda: [0]*len(terms))\n","\n","  # Vector per query\n","  queryVector = [0]*len(terms)\n","\n","  # TF of query\n","  query_terms_count = collections.Counter(terms)\n"," \n","  # Norm query\n","  query_norm = np.linalg.norm(list(query_terms_count.values()))\n","  \n","  for termIndex, term in enumerate(terms):\n","    # Check if term exist in collection\n","\n","    if term not in index:\n","      continue\n","\n","    # Score per term-query\n","    queryVector[termIndex] = query_terms_count[term]/query_norm * idf[term]\n","\n","    for docIndex, (doc,postings) in enumerate(index[term]):\n","      # check if IDdoc is in list of IDdocs containg term\n","      if doc in docs:\n","        # Score per term-doc\n","        docVectors[doc][termIndex] = tf[term][docIndex] * idf[term]\n","\n","  #Cosine similarity query-doc\n","  docScores = [[np.dot(curDocVec, queryVector), doc] for doc, curDocVec in docVectors.items()]\n","\n","  #Sort by descending similarity\n","  docScores.sort(reverse=True)\n","\n","  #Get IDs\n","  resultDocs = [x[1] for x in docScores]\n","\n","  return resultDocs"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"TAn5wlHGfANu","executionInfo":{"status":"ok","timestamp":1607550106365,"user_tz":-60,"elapsed":1029434,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["def search_tf_idf(query, index, idf, tf, topn):\n","  \"\"\"\n","Preprocess query and find docs with words in query\n","Arguments:\n","  query -- query - str.\n","  index -- inverted index - dict\n","  idf -- inverse document frequency - dict\n","  tf -- term frequency - dict\n","  topn -- N top ranked docs to be returned - int\n","Returns\n","  ranked_docs -- list of topn docs ranked by cosine-sim - list\n","  \"\"\"\n","  # Preprocess query\n","  query = getTerms(query)\n"," \n","  # Init set of docs with terms in query\n","  docs = set()\n","\n","  for term in query:\n","    try:\n","      # Get IDs of docs with term\n","      termDocs = [posting[0] for posting in index[term]]\n","      \n","      # Add new docsID\n","      docs = docs.union(termDocs)\n","    \n","    except:\n","      pass\n","    \n","  docs = list(docs)\n","\n","  # Rank docs with rankDocuments\n","  ranked_docs = rankDocuments(query, docs, index, idf, tf)\n","  ranked_docs = ranked_docs[:topn]\n","\n","  return ranked_docs"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"YbV01Ka_wc3x","executionInfo":{"status":"ok","timestamp":1607550106366,"user_tz":-60,"elapsed":1029256,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["def parser_tweet_results(doc):\n","  \"\"\"\n","Given a Pandas dataframe row formates the information por display\n","Arguments:\n","  docs -- pandas dataframe with unique row with tweet info.\n","Returns:\n","  tweet -- text tweet - str\n","  authors -- user name of tweet - str\n","  date -- of publication -- str\n","  retweets -- count of retweets - str\n","  favorites -- count of favourites - str\n","  \"\"\"\n","  # Tweet\n","  tweet = str(doc[\"full_text\"].values)\n","  tweet = tweet.replace(\"'\",\"\")\n","  tweet = tweet.replace(\"[\",\"\")\n","  tweet = tweet.replace(\"]\",\"\")\n","\n","  # Author\n","  author = str(doc[\"user.name\"].values)\n","  author = author.replace(\"[\",\"\")\n","  author = author.replace(\"]\",\"\")\n","\n","  # Date\n","  date = str(doc[\"created_at\"].values)\n","  date = date.replace(\"[\",\"\")\n","  date = date.replace(\"]\",\"\")\n","  date = date.replace(\"'\",\"\")\n","\n","  # Retweets\n","  retweets = str(doc[\"retweet_count\"].values)\n","  retweets = retweets.replace(\"[\",\"\")\n","  retweets = retweets.replace(\"]\",\"\")\n","\n","  # Favorites\n","  favorites = str(doc[\"favorite_count\"].values)\n","  favorites = favorites.replace(\"[\",\"\")\n","  favorites = favorites.replace(\"]\",\"\")\n","\n","  # URL\n","  id = str(doc[\"id\"].values)\n","  id = id.replace(\"[\",\"\")\n","  id = id.replace(\"]\",\"\")\n","  url = f\"https://twitter.com/twitter/statuses/{id}\"\n","\n","  #Hashtags\n","  hashtags = str(doc[\"entities.hashtags\"].values)\n","\n","  return tweet, date, author, retweets, favorites, url, hashtags"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"qaQIAOOFpbse","executionInfo":{"status":"ok","timestamp":1607550106367,"user_tz":-60,"elapsed":1029068,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["def search(index, idf, tf, topn = 20):\n","  \"\"\"\n","Search for tweets inputing a query and see displayed results.\n","Arguments:\n","  index -- inverted index - dict\n","  idf -- inverse document frequency - dict.\n","  tf -- term frequency - dict.\n","  topn -- default: 20 - Top N result to display - int.\n","  \"\"\"\n","  print(\"######################################################\")\n","  print(\"Insert query:\")\n","  query = input()\n","  print(\"######################################################\\n\")\n","  \n","  # Get topn docs\n","  ranked_docs = search_tf_idf(query, index, idf, tf, topn)\n","\n","  if len(ranked_docs) == 0:\n","    print(\"No results found !\")\n","    return -1\n","  \n","  print(\"Results\\n\")\n","\n","  for i, id in enumerate(ranked_docs):\n","    # Get tweet corresponding to id\n","    doc = data[data['id'] == id]\n","    tweet, date, author, retweets, favorites, url, hashtags = parser_tweet_results(doc)\n","    \n","    print(\"______________________________________________________\")\n","    print(f\"Tweet {i}\")\n","    print(f\"\\t·Author: {author}\")\n","    print(f\"\\t·Date: {date}\")\n","    print(f\"\\t·Tweet: {tweet}\")\n","    print(f\"\\t·Retweets: {retweets}\")\n","    print(f\"\\t·Favorites: {favorites}\")\n","    print(f\"\\t·Hashtags: {hashtags}\")\n","    print(f\"\\t·URL: {url}\")\n","    print(\"______________________________________________________\\n\")"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"code","id":"MHdpmm1jgLES","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1dda7836-b989-46d6-a4de-c7eacfba2920"},"source":["search(index, idf, tf)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["######################################################\n","Insert query:\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RWQelQxLvVh7"},"source":[""],"execution_count":null,"outputs":[]}]}
