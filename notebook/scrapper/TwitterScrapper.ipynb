{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TwitterScrapper.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOBsKhJxCh7t/dSNy5sauft"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ORZiyqiStNmY"},"source":["  Gabriel Graells Sol√© - 205638\n","# Twitter Scrapper - 2020 USA Elections\n","\n","The goal of this Notebook is to scrappe Tweets related to the election of USA on 2020. To do so we will do a stream search using Twitter's API and a set of keywords related to this topic. We will use **Tweepy** to easy the work with Tweeters API.\n","\n","The elections ended the Tuesday, November 3, 2020."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_aeKlDytGil","executionInfo":{"status":"ok","timestamp":1606584746389,"user_tz":-60,"elapsed":18230,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}},"outputId":"eaf68089-bb99-4e2a-8c47-76c80195755f"},"source":["import json\n","import pandas as pd\n","import numpy as np\n","import datetime\n","import csv\n","from collections import Counter\n","from tweepy.streaming import StreamListener\n","from tweepy import OAuthHandler\n","from tweepy import Stream\n","from tweepy import API\n","from tweepy import Cursor\n","import time\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","PATH = '/content/drive/My Drive/Colab Notebooks/IR-WA_FinalProject/data/'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lF1Bhs2suJz-"},"source":["## Connect to the API"]},{"cell_type":"code","metadata":{"id":"S3jp2X0jt8MP","executionInfo":{"status":"ok","timestamp":1606584746390,"user_tz":-60,"elapsed":18220,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["# Access token informations \n","access_token1 = \"1330533182681313288-yMxmNMMhN5pnDKzfRBRCK4XXcpeYvO\"\n","access_token_secret1 = \"mNOFxiv6JnjV4ze0EiuhXT4xSDkNAQ04NhKdB5g7uTycx\"\n","\n","consumer_key1 = \"sDFQbByaPblJTMYaPzcX09sdv\"\n","consumer_secret1 = \"RoMLGdY3hgfyzZbj1U5x4BN5K8gIgk6lUmwJZA4o9ak0gotLlu\""],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Voqo8vQuD3r","executionInfo":{"status":"ok","timestamp":1606584746391,"user_tz":-60,"elapsed":18213,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["# Connect to Twitter API\n","auth = OAuthHandler(consumer_key1, consumer_secret1)\n","auth.set_access_token(access_token1, access_token_secret1)\n","api = API(auth_handler=auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zeR8YC518s4j"},"source":["## Scrape News Accounts\n","\n","Some accounts can provide more trust-worthy information than others, for example news papers accounts should provide more usefull and trust worthy information than random users. Thus in this section we will retrieve tweets from news paper from the USA and internation press.\n","\n","We want to **filter tweets per day**. To do so we define a window of days in which we want to select tweets. The elections accorred the 03-11-2020 we will take tweets from a week and two days after and a week previous to election day. Then the range of days is the following **27-10-2020 to 12-11-2020**. \n","\n","Filtering is a must, since most of the recent tweets are not related to this topic, they are irrelevant for our search engine. Having a collection which contains only relevant docs regarding USA election topic will increase the recall and reduce search time."]},{"cell_type":"code","metadata":{"id":"RvrRUWKw9BCK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606584972690,"user_tz":-60,"elapsed":100327,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}},"outputId":"4e034b8e-6710-4d49-bae1-e0263db90f35"},"source":["# News twitter account\n","users = [\"@GuardianUS\",\"@CNNPolitics\",\"@nytimes\",\"@washingtonpost\"\n","         ,\"@HuffPostPol\",\"@bpolitics\"]\n","\n","\n","# Use attributes\n","attributes = [\"created_at\",\"id\", \"full_text\", \"favorite_count\",\"retweet_count\",\"user.id\",\"user.name\"]\n","\n","# Init Dataframe\n","df_news = pd.DataFrame()\n","\n","# Dates window\n","start_date = datetime.datetime(2020, 10, 27, 0,0,0)\n","end_date = datetime.datetime(2020, 11, 12, 0,0,0)\n","\n","for user in users:\n","\n","  # Store tweets\n","  tweets = []\n","\n","  # Select tweets within range\n","  for status in Cursor(api.user_timeline, screen_name = user,tweet_mode=\"extended\", lang = \"en\").items():\n","    if status.created_at <= end_date and status.created_at >= start_date:\n","      tweets.append(status)\n","    elif status.created_at < start_date:\n","      break\n","  \n","  # Get user time line data in json form\n","  json_data = [t._json for t in tweets]\n","  \n","  # Create df per user\n","  df = pd.io.json.json_normalize(json_data)\n","\n","  # Get only needed attributes\n","  df = df[np.intersect1d(df.columns, attributes)]\n"," \n","  # Change created_at datetime format\n","  df['created_at'] = df['created_at'].apply(lambda x: datetime.datetime.strftime(datetime.datetime.strptime(x,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d'))\n","  \n","  # Append to final df\n","  df_news = df_news.append(df)\n","  \n","  # Sleep\n","  time.sleep(1)\n","\n","# Reset index \n","df_news.index = np.arange(len(df_news))\n","\n","# Save to csv\n","df_news.to_csv(f\"{PATH}news_Tweets.csv\", index = False)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"tvFNp7Fdb1YK"},"source":["# Scrape Political Agents\n","\n","We will also scrape data from political agents in the USA. Again the goal is to find entities that are more trust worthy and thus we assume that their tweets would be much more relevant for the queries."]},{"cell_type":"code","metadata":{"id":"K-I_dVUKd2_l"},"source":["# List of user names\n","politicians = [\"@JoeBiden\",\"@KamalaHarris\",\"@BernieSanders\",\"@BarackObama\", \"@realDonaldTrump\"]\n","\n","# List of attributes\n","attributes = [\"created_at\",\"id\", \"full_text\", \"favorite_count\",\"retweet_count\",\"user.id\",\"user.name\"]\n","\n","# Final dataframe\n","df_politicians = pd.DataFrame()\n","\n","# Dates window\n","start_date = datetime.datetime(2020, 10, 27, 0,0,0)\n","end_date = datetime.datetime(2020, 11, 12, 0,0,0)\n","\n","for politician in politicians:\n","\n","  # Store tweets\n","  tweets = []\n","\n","  # Select tweets within range\n","  for status in Cursor(api.user_timeline, screen_name = politician, tweet_mode=\"extended\", lang = \"eng\").items():\n","    if status.created_at <= end_date and status.created_at >= start_date:\n","      tweets.append(status)\n","    elif status.created_at < start_date:\n","      break\n","  \n","  # Get user time line data in json form\n","  json_data = [t._json for t in tweets]\n","  \n","  # Create df per user\n","  df = pd.io.json.json_normalize(json_data)\n","  print(politician)\n","  # Get only needed attributes\n","  df = df[np.intersect1d(df.columns, attributes)]\n"," \n","  # Change created_at datetime format\n","  df['created_at'] = df['created_at'].apply(lambda x: datetime.datetime.strftime(datetime.datetime.strptime(x,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d'))\n","\n","  # append to final df\n","  df_politicians = df_politicians.append(df)\n","\n","# Reset index\n","df_politicians.index = np.arange(len(df_politicians))\n","\n","#Save to csv\n","df_politicians.to_csv(f\"{PATH}politicians_Tweets.csv\", index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NtPBWTNZxZ0c"},"source":["## Scrape Hashtags\n","\n","Selecting tweets from a stream it is not the best practice to get relevant data, particularly in this topic. Due the debate on the validity of the elections results many users did not tweet the reality of the elections rather their opinion on what was happening. For example if we issue the query: who won the elections? the resulting docs may be missleading since many Trump voters have posted tweets that do not follow the truth. \n","\n","One issue that we are facing is the fact that Twitter Standard API only allows to search for tweets created at a maximum interval of 7 days ago from the request date. Thus our search by hashtag is limited to this constrain.\n","\n","**IMPORTANT**\n","Twitter has paramter **filter_level** which is used to rate relevance of tweets. **filter_level** takes values [\"low\", \"medium\", \"high\"]"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Pv1zAMC0b-N","executionInfo":{"status":"ok","timestamp":1606561006347,"user_tz":-60,"elapsed":31569,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}},"outputId":"6fc9051f-e40b-46e4-d4b3-0a506caf82c3"},"source":["# Hashtags to search\n","hashtags = [\"#USAElections2020\", \"#uselections\", \"#USElections\", \n","            \"#2020election\", \"#2020Election\",\"#USElectionResults\",\n","            \"#vote\",\"#voteblue\", \"#berniesanders\",\"#donaldtrump\"]\n","\n","# Attributes\n","attributes = [\"created_at\",\"id\", \"full_text\", \"favorite_count\",\"retweet_count\",\"user.id\",\"user.name\"]\n","\n","# Init dataframe\n","df_hashtags = pd.DataFrame()\n","\n","# Number of tweets per hashtag\n","count = 200\n","\n","for h in hashtags:\n","  tweets = []\n","\n","  for status in Cursor(api.search, q = h, tweet_mode = \"extended\", lang = \"en\").items(count):\n","    tweets.append(status)\n","\n","  # Get status data in json form\n","  json_data = [t._json for t in tweets]\n","\n","  # Create df per hashtag\n","  df = pd.io.json.json_normalize(json_data)\n","\n","  # Get only needed attributes\n","  df = df[np.intersect1d(df.columns, attributes)]\n"," \n","  # Change created_at datetime format\n","  df['created_at'] = df['created_at'].apply(lambda x: datetime.datetime.strftime(datetime.datetime.strptime(x,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d'))\n","\n","  # Append to final df\n","  df_hashtags = df_hashtags.append(df)\n","\n","# Reset index\n","df_hashtags.index = np.arange(len(df_hashtags))\n","\n","# Save to csv\n","df_hashtags.to_csv(f\"{PATH}hashtags_Tweets.csv\", index = False)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"7gSVWLAvjVBX"},"source":["## Merge DataFrames"]},{"cell_type":"code","metadata":{"id":"q2ZIbVawjUPI","executionInfo":{"status":"ok","timestamp":1606573122694,"user_tz":-60,"elapsed":787,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["# Read df\n","df_news = pd.read_csv(f\"{PATH}news_Tweets.csv\")\n","df_politicians = pd.read_csv(f\"{PATH}politicians_Tweets.csv\")\n","df_hashtags = pd.read_csv(f\"{PATH}hashtags_Tweets.csv\")\n","\n","# Concatenate df\n","df_final = pd.concat([df_news,df_politicians,df_hashtags])\n","\n","# Remove duplicates\n","df_final = df_final.drop_duplicates ()\n","\n","# Reset index\n","df_final.index = np.arange(len(df_final))\n","\n","# Save in csv\n","df_final.to_csv(f\"{PATH}final_Tweets.csv\", index = False)"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiz-Y-wlNUZN"},"source":[""],"execution_count":null,"outputs":[]}]}