{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RQ2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMBfD3JJxRHD+EGtMT9CNMV"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8p5tkkLr6AtY","executionInfo":{"status":"ok","timestamp":1607422328356,"user_tz":-60,"elapsed":668,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}},"outputId":"0e0e1032-27a4-46ab-91ad-1d078dea83e9"},"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from array import array\n","import collections\n","from collections import defaultdict\n","import string\n","import pickle\n","import random\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","PATH = '/content/drive/My Drive/Colab Notebooks/IR-WA_FinalProject/'"],"execution_count":115,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uTBwsxit6Erp"},"source":["# RQ2 - Report Script\n","\n","A notebook containing all the required scripts to answer some of the report questions.\n","\n","---"]},{"cell_type":"code","metadata":{"id":"6oSNu09hMIVM","executionInfo":{"status":"ok","timestamp":1607420153879,"user_tz":-60,"elapsed":612,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["# Open data\n","data = pd.read_csv(f\"{PATH}data/final_Tweets.csv\")\n","\n","#Open labels\n","id_labels = pickle.load(open(f\"{PATH}ResearchQuestions/id_cluster.p\",\"rb\"))\n","ids = list(id_labels.keys())"],"execution_count":95,"outputs":[]},{"cell_type":"code","metadata":{"id":"dB_udQoQNUyx","executionInfo":{"status":"ok","timestamp":1607420154204,"user_tz":-60,"elapsed":736,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["# Init labels\n","data = (data[ data[\"id\"].isin(ids) ]).reset_index()\n","data[\"Label\"] = -2"],"execution_count":96,"outputs":[]},{"cell_type":"code","metadata":{"id":"tObFqNggczWC","executionInfo":{"status":"ok","timestamp":1607420158792,"user_tz":-60,"elapsed":5136,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["# Assign labels\n","for i, row in data.iterrows():\n","  row_id = row[\"id\"]\n","  try:\n","    label = id_labels[row_id]\n","    data.loc[i,\"Label\"]= label\n","  except:\n","    pass"],"execution_count":97,"outputs":[]},{"cell_type":"code","metadata":{"id":"1h3zNEQOgHh6","executionInfo":{"status":"ok","timestamp":1607420159717,"user_tz":-60,"elapsed":917,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["# Get labeled data\n","data = data[data[\"Label\"] != -2]"],"execution_count":98,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HOEr9QqPlc2X"},"source":["# TF-IDF + Cosine Similarity"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJ7dI_ylmZkw","executionInfo":{"status":"ok","timestamp":1607418356134,"user_tz":-60,"elapsed":580,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}},"outputId":"665e69aa-cf1c-4112-946e-12292de0f385"},"source":["nltk.download('stopwords')"],"execution_count":67,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"w4KEIGvcyU5d","executionInfo":{"status":"ok","timestamp":1607421690817,"user_tz":-60,"elapsed":609,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["# Inverse Probability of each cluster\n","P_cluster = {}\n","\n","#Unique clusters\n","clusters = set(id_labels.values())\n","\n","numDocs = len(data)\n","\n","for cluster in clusters:\n","  num_in_cluster = len(data[data[\"Label\"] == cluster])\n","  P_cluster[cluster] = numDocs/num_in_cluster"],"execution_count":108,"outputs":[]},{"cell_type":"code","metadata":{"id":"4aPx-YPckym-","executionInfo":{"status":"ok","timestamp":1607420159718,"user_tz":-60,"elapsed":910,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["\n","def getTerms(terms):\n","\n","  \"\"\"\n","  Preprocess step for tweet.\n","    1. transform all text in lowercase\n","    2. Tokenize by transforming string to list\n","    3. Remove stopwords\n","    4. Apply stemming, simplify word to root word.\n","  \n","  Argument:\n","    terms -- string (text)\n","  \n","  Returns:\n","    terms -- a list of preprocessed tokens corresponding to the input tweet \n","  \"\"\"\n","  # Init stemming and stopwords\n","  stemming = PorterStemmer()\n","  STOPWORDS = set(stopwords.words(\"english\"))\n","\n","  # Transform into lower case\n","  terms = terms.lower() \n","\n","  # Remove punctuation\n","  terms = terms.translate(str.maketrans(\"\",\"\", string.punctuation))\n","\n","  # Tokenize\n","  terms = terms.split()\n","\n","  # Remove stop words\n","  terms = [t for t in terms if t not in STOPWORDS]\n","\n","  #Stemming\n","  terms = [stemming.stem(t) for t in terms]\n","\n","  # Remove http links found in tweets last term\n","  try:\n","    if terms[-1][:4] == \"http\":\n","     terms = terms[:-1]\n","  except:\n","    pass\n","\n","  return terms"],"execution_count":99,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pDRA015mDyb","executionInfo":{"status":"ok","timestamp":1607420159719,"user_tz":-60,"elapsed":893,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["def create_index_tfidf(data, numDocs):\n","  \"\"\"\n","    Creates index and computes tf, idf and df for collection\n","\n","    Arguments:\n","      data -- Pandas dataframe with cols \"full_index\" per text tweet and \"id\" per \n","              tweet-\n","    Returns:\n","      index --  inverted list \"term\": [[\"id\",[pos1,pos1,..]]..]\n","      tf -- normalized term frequency per doc \n","      df -- document frequency per term\n","      idf -- inversed docuemnt frequency\n","  \"\"\"\n","\n","  index = defaultdict(list)\n","\n","  # Term freq of terms in tweets\n","  tf = defaultdict(list)\n","\n","  # Tweet freq of term in corpus\n","  df = defaultdict(int)\n","\n","  # Inverse df\n","  idf = defaultdict(float)\n","\n","  for i, row in data.iterrows():\n","    tweet = row[\"full_text\"]\n","    tweet = getTerms(tweet)\n","    tweet_id = row[\"id\"]\n","\n","    # Page Index per doc\n","    termdictPage = {}\n","    for position, term in enumerate(tweet):\n","      try:\n","        termdictPage[term][1].append(position)\n","      except:\n","        termdictPage[term] = [tweet_id, array(\"I\",[position])]\n","    \n","    # NORM frequencies per doc \n","    norm = 0\n","    for term, posting in termdictPage.items():\n","      norm += len(posting[1])**2\n","    norm = np.sqrt(norm)\n","\n","    # TF normalized per term and DF\n","    for term, posting in termdictPage.items():\n","      tf[term].append(np.round(len(posting[1])/norm,4))\n","      df[term] += 1\n","    \n","    # Merge doc page index with main index\n","    for termpage, postingpage in termdictPage.items():\n","      index[termpage].append(postingpage)\n","\n","    # IDF per term\n","    for term in df:\n","      idf[term] = np.round(np.log(float(numDocs/df[term])),4)\n","  \n","  return index, tf, df, idf"],"execution_count":101,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HEomqmvgmG-J","executionInfo":{"status":"ok","timestamp":1607421184754,"user_tz":-60,"elapsed":1025538,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}},"outputId":"8f26a092-8318-46ae-c252-46b1d4cb8669"},"source":["%%time\n","\n","numDocs = len(data)\n","index, tf, df, idf = create_index_tfidf(data, numDocs)"],"execution_count":102,"outputs":[{"output_type":"stream","text":["CPU times: user 16min 59s, sys: 1.58 s, total: 17min 1s\n","Wall time: 17min 4s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kzR_rmosmQm-","executionInfo":{"status":"ok","timestamp":1607427281544,"user_tz":-60,"elapsed":1036,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["def rankDocuments(terms, docs, index, idf, tf):\n","  \"\"\"\n","  Computes ranking given query and collection of tweets.\n","\n","  Arguments:\n","    terms -- query - str.\n","    docs -- ID list of docs - list.\n","    index -- invertex index. - dict\n","    idf -- inverse document frequency - dict\n","    tf -- term frequency - dict\n","  Returns:\n","    resultDocs -- Ordered list of matching docs based on cosine-sim - list\n","  \"\"\"\n","\n","  # Dict with vector per docID\n","  docVectors = defaultdict(lambda: [0]*len(terms))\n","\n","  # Vector per query\n","  queryVector = [0]*len(terms)\n","\n","  # TF of query\n","  query_terms_count = collections.Counter(terms)\n"," \n","  # Norm query\n","  query_norm = np.linalg.norm(list(query_terms_count.values()))\n","  \n","  for termIndex, term in enumerate(terms):\n","    # Check if term exist in collection\n","    if term not in index:\n","      continue\n","\n","    # Score per term-query\n","    queryVector[termIndex] = query_terms_count[term]/query_norm * idf[term]\n","\n","    for docIndex, (doc,postings) in enumerate(index[term]):\n","      # check if IDdoc is in list of IDdocs containg term\n","      if doc in docs:\n","        \n","        # Score per term-doc\n","        docVectors[doc][termIndex] = tf[term][docIndex] * idf[term]\n","\n","  #Cosine similarity query-doc\n","  docScores = [[np.dot(curDocVec, queryVector), doc] for doc, curDocVec in docVectors.items()]\n","\n","  #Sort by descending similarity\n","  docScores.sort(reverse=True)\n","\n","  # Transform to dict\n","  docScore_dic = {}\n","  for d in docScores:\n","    docScore_dic[d[1]] = d[0]\n","\n","  #Get IDs\n","  resultDocs = [x[1] for x in docScores]\n","  \n","  diversity = 0\n","  final_resultDocs = []\n","\n","  if diversification == True:\n","    #Create permutation\n","    for i in range(20):\n","      # Get top 40 docs\n","      resultDocs_ = resultDocs[:50]\n","\n","      # Permute\n","      random.shuffle(resultDocs_)\n","      \n","      resultDocs_ = resultDocs_[:20]\n","\n","      # Get data in permutation\n","      docs_in_permutation = data[data[\"id\"].isin(resultDocs_)]\n","\n","      #Coverage\n","      clusters_in_permutation = docs_in_permutation[\"Label\"]\n","      coverage = len(set(clusters_in_permutation))\n","    \n","      #Cosine Sim\n","      cosine_sim = sum([docScore_dic[id] for id in resultDocs_])*0.1\n","      \n","      #Inverse cluster prob\n","      lambda_ = 0.01\n","      P_clusters_in_permutation = sum([P_cluster[c] for c in clusters_in_permutation])*lambda_\n","      \n","      #Diversity\n","      diversity_ = coverage + cosine_sim + P_clusters_in_permutation\n","\n","      if diversity_ > diversity:\n","        diversity = diversity_\n","\n","        dic = {}\n","        for d in resultDocs_:\n","          dic[d] = docScore_dic[d]\n","        \n","        dic = {k: v for k, v in sorted(dic.items(), key=lambda item: item[1], reverse=True)}\n","        \n","        final_resultDocs = list(dic.keys())\n","  else:\n","    final_resultDocs = resultDocs[:20]\n","    docs_in_result = data[data[\"id\"].isin(final_resultDocs)]\n","\n","    #Coverage\n","    clusters_in_result = docs_in_result[\"Label\"]\n","    coverage = len(set(clusters_in_result))\n","    # Cos Sim\n","    cosine_sim = sum([docScore_dic[id] for id in final_resultDocs])*0.1\n","    # Invser cluster prob\n","    lambda_ = 0.01\n","    P_cluster_in_result = sum([P_cluster[c] for c in clusters_in_result])*lambda_\n","    # Diversity \n","    diversity = coverage + cosine_sim + P_cluster_in_result\n","\n","  print(f\"Coverage: {coverage}\")\n","  print(f\"Diversity: {diversity}\")\n","  return final_resultDocs"],"execution_count":247,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNy1o9OtsNh-","executionInfo":{"status":"ok","timestamp":1607427281946,"user_tz":-60,"elapsed":1370,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["def search_tf_idf(query, index, idf, tf, topn):\n","  \"\"\"\n","Preprocess query and find docs with words in query\n","Arguments:\n","  query -- query - str.\n","  index -- inverted index - dict\n","  idf -- inverse document frequency - dict\n","  tf -- term frequency - dict\n","  topn -- N top ranked docs to be returned - int\n","Returns\n","  ranked_docs -- list of topn docs ranked by cosine-sim - list\n","  \"\"\"\n","  # Preprocess query\n","  query = getTerms(query)\n"," \n","  # Init set of docs with terms in query\n","  docs = set()\n","\n","  for term in query:\n","    try:\n","      # Get IDs of docs with term\n","      termDocs = [posting[0] for posting in index[term]]\n","      \n","      # Add new docsID\n","      docs = docs.union(termDocs)\n","    \n","    except:\n","      pass\n","    \n","  docs = list(docs)\n","\n","  # Rank docs with rankDocuments\n","  ranked_docs = rankDocuments(query, docs, index, idf, tf)\n","  ranked_docs = ranked_docs[:topn]\n","\n","  return ranked_docs"],"execution_count":248,"outputs":[]},{"cell_type":"code","metadata":{"id":"_kz8Ami6sQM-","executionInfo":{"status":"ok","timestamp":1607427357732,"user_tz":-60,"elapsed":775,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["def parser_tweet_results(doc):\n","  \"\"\"\n","Given a Pandas dataframe row formates the information por display\n","Arguments:\n","  docs -- pandas dataframe with unique row with tweet info.\n","Returns:\n","  tweet -- text tweet - str\n","  authors -- user name of tweet - str\n","  date -- of publication -- str\n","  retweets -- count of retweets - str\n","  favorites -- count of favourites - str\n","  \"\"\"\n","  # Tweet\n","  tweet = str(doc[\"full_text\"].values)\n","  tweet = tweet.replace(\"'\",\"\")\n","  tweet = tweet.replace(\"[\",\"\")\n","  tweet = tweet.replace(\"]\",\"\")\n","\n","  # Author\n","  author = str(doc[\"user.name\"].values)\n","  author = author.replace(\"[\",\"\")\n","  author = author.replace(\"]\",\"\")\n","\n","  # Date\n","  date = str(doc[\"created_at\"].values)\n","  date = date.replace(\"[\",\"\")\n","  date = date.replace(\"]\",\"\")\n","  date = date.replace(\"'\",\"\")\n","\n","  # Retweets\n","  retweets = str(doc[\"retweet_count\"].values)\n","  retweets = retweets.replace(\"[\",\"\")\n","  retweets = retweets.replace(\"]\",\"\")\n","\n","  # Favorites\n","  favorites = str(doc[\"favorite_count\"].values)\n","  favorites = favorites.replace(\"[\",\"\")\n","  favorites = favorites.replace(\"]\",\"\")\n","\n","  # URL\n","  id = str(doc[\"id\"].values)\n","  id = id.replace(\"[\",\"\")\n","  id = id.replace(\"]\",\"\")\n","  url = f\"https://twitter.com/twitter/statuses/{id}\"\n","\n","  #Hashtags\n","  hashtags = str(doc[\"entities.hashtags\"].values)\n","\n","  return tweet, date, author, retweets, favorites, url, hashtags"],"execution_count":252,"outputs":[]},{"cell_type":"code","metadata":{"id":"m88DfnJGQLdJ","executionInfo":{"status":"ok","timestamp":1607431109935,"user_tz":-60,"elapsed":594,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["def search(index, idf, tf, topn = 20):\n","  \"\"\"\n","Search for tweets inputing a query and see displayed results.\n","Arguments:\n","  index -- inverted index - dict\n","  idf -- inverse document frequency - dict.\n","  tf -- term frequency - dict.\n","  topn -- default: 20 - Top N result to display - int.\n","  \"\"\"\n","  print(\"######################################################\")\n","  print(\"Insert query:\")\n","  querys = [\"Who won the elections?\", \"Votes count\", \"Is Joe Biden the president?\", \"Pennsylvania votes\", \"Georgia votes\", \"Texas votes\", \"Kamala Harris\", \"USA elections date\", \"Final recount day\", \"How to boil water\"]\n","  print(\"######################################################\\n\")\n","  \n","  # Get topn docs\n","  df = pd.DataFrame()\n","  for query in querys:\n","    ranked_docs = search_tf_idf(query, index, idf, tf, topn)\n","\n","    if len(ranked_docs) == 0:\n","      print(\"No results found !\")\n","      return -1\n","    \n","    print(\"Results\\n\")\n","\n","    for i, id in enumerate(ranked_docs):\n","      # Get tweet corresponding to id\n","      doc = data[data['id'] == id]\n","      tweet, date, author, retweets, favorites, url, hashtags = parser_tweet_results(doc)\n","      \n","      dic = {\n","          \"query\":query,\n","          \"tweet\":tweet,\n","          \"date\":date,\n","          \"retweets\":retweets,\n","          \"favorites\":favorites,\n","          \"url\":url,\n","          \"hashtags\":hashtags\n","      }\n","\n","      df = df.append(dic, ignore_index = True)\n","  \n","  df.to_csv(f\"{PATH}ResearchQuestions/RQ2_WITHOUT.csv\")"],"execution_count":283,"outputs":[]},{"cell_type":"code","metadata":{"id":"x9AhFlc7sSbu","executionInfo":{"status":"ok","timestamp":1607427357733,"user_tz":-60,"elapsed":494,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":["def search(index, idf, tf, topn = 20):\n","  \"\"\"\n","Search for tweets inputing a query and see displayed results.\n","Arguments:\n","  index -- inverted index - dict\n","  idf -- inverse document frequency - dict.\n","  tf -- term frequency - dict.\n","  topn -- default: 20 - Top N result to display - int.\n","  \"\"\"\n","  print(\"######################################################\")\n","  print(\"Insert query:\")\n","  query = input()\n","  print(\"######################################################\\n\")\n","  \n","  # Get topn docs\n","  ranked_docs = search_tf_idf(query, index, idf, tf, topn)\n","\n","  if len(ranked_docs) == 0:\n","    print(\"No results found !\")\n","    return -1\n","  \n","  print(\"Results\\n\")\n","\n","  for i, id in enumerate(ranked_docs):\n","    # Get tweet corresponding to id\n","    doc = data[data['id'] == id]\n","    tweet, date, author, retweets, favorites, url, hashtags = parser_tweet_results(doc)\n","    \n","    print(\"______________________________________________________\")\n","    print(f\"Tweet {i}\")\n","    print(f\"\\t·Author: {author}\")\n","    print(f\"\\t·Date: {date}\")\n","    print(f\"\\t·Tweet: {tweet}\")\n","    print(f\"\\t·Retweets: {retweets}\")\n","    print(f\"\\t·Favorites: {favorites}\")\n","    print(f\"\\t·Hashtags: {hashtags}\")\n","    print(f\"\\t·URL: {url}\")\n","    print(\"______________________________________________________\\n\")"],"execution_count":253,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYpMVdczHMKQ","executionInfo":{"status":"ok","timestamp":1607431114239,"user_tz":-60,"elapsed":1988,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}},"outputId":"e1299dbd-1dfd-46aa-f3a1-3a082d49eaa7"},"source":["diversification = False\n","\n","search(index, idf, tf, topn=20)"],"execution_count":284,"outputs":[{"output_type":"stream","text":["######################################################\n","Insert query:\n","######################################################\n","\n","Coverage: 7\n","Diversity: 34.82464988728815\n","Results\n","\n","Coverage: 8\n","Diversity: 66.78907984179534\n","Results\n","\n","Coverage: 6\n","Diversity: 54.01390667735626\n","Results\n","\n","Coverage: 6\n","Diversity: 49.58758875155985\n","Results\n","\n","Coverage: 7\n","Diversity: 71.98763742592809\n","Results\n","\n","Coverage: 5\n","Diversity: 39.32855082276729\n","Results\n","\n","Coverage: 7\n","Diversity: 46.27009432401513\n","Results\n","\n","Coverage: 2\n","Diversity: 42.5010623032415\n","Results\n","\n","Coverage: 9\n","Diversity: 55.86148093788917\n","Results\n","\n","Coverage: 5\n","Diversity: 28.095314125300057\n","Results\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_q-9rA1ts7k1","executionInfo":{"status":"ok","timestamp":1607427572484,"user_tz":-60,"elapsed":739,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}}},"source":[""],"execution_count":257,"outputs":[]}]}